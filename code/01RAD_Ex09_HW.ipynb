{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CasualMathEnjoyer/01RAD_Ex09_HW/blob/main/code/01RAD_Ex09_HW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01RAD  Exercise 09 - Homework\n",
        "\n",
        "Authors: Your names here  \n",
        "Date: 2024-12-03  \n",
        "\n",
        "---\n",
        "\n",
        "## Task Description\n",
        "\n",
        "The dataset is based on the **House Sales in King County, USA** dataset, which can be found, for example, on kaggle.com or in the `moderndive` library under the name `house_prices`. The original dataset contains house sale prices in the King County area, which includes Seattle, and the data was collected between May 2014 and May 2015. For our purposes, several variables have been removed, and the dataset has been significantly reduced and slightly modified.\n",
        "\n",
        "The dataset has already been split into three parts and modified, all of which will be used progressively throughout this assignment.\n",
        "\n",
        "---\n",
        "\n",
        "## Variables Description\n",
        "\n",
        "The dataset contains the following 18 variables, and our goal is to explore the influence of 12 of them on the target variable `price`.\n",
        "\n",
        "| Feature         | Description                                           |\n",
        "|------------------|-------------------------------------------------------|\n",
        "| `id`            | Unique identifier for a house                         |\n",
        "| `price`         | Sale price (prediction target)                        |\n",
        "| `bedrooms`      | Number of bedrooms                                    |\n",
        "| `bathrooms`     | Number of bathrooms                                   |\n",
        "| `sqft_living`   | Square footage of the home                            |\n",
        "| `sqft_lot`      | Square footage of the lot                             |\n",
        "| `floors`        | Total number of floors (levels) in the house          |\n",
        "| `waterfront`    | Whether the house has a waterfront view               |\n",
        "| `view`          | Number of times the house has been viewed             |\n",
        "| `condition`     | Overall condition of the house                        |\n",
        "| `grade`         | Overall grade given to the housing unit               |\n",
        "| `sqft_above`    | Square footage of the house apart from the basement   |\n",
        "| `sqft_basement` | Square footage of the basement                        |\n",
        "| `yr_built`      | Year the house was built                              |\n",
        "| `yr_renovated`  | Year when the house was renovated                     |\n",
        "| `sqft_living15` | Living room area in 2015 (after renovations)          |\n",
        "| `sqft_lot15`    | Lot size in 2015 (after renovations)                  |\n",
        "| `split`         | Splitting variable with train, test, and validation samples |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "WQJA5t5UlK9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/francji1/01RAD/main/data/01RAD_2024_house.csv\"\n",
        "house_rad = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "house_rad.head()\n"
      ],
      "metadata": {
        "id": "b4IQE-BBl5S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Exploratory and Graphical Analysis\n",
        "\n",
        "### Question 1\n",
        "\n",
        "Verify the dimensions of the dataset, the types of individual variables, and summarize the basic descriptive statistics of all variables. Plot a histogram and a density estimate for the target variable `price`. Can anything be inferred for future analysis?\n",
        "\n",
        "---\n",
        "\n",
        "### Question 2\n",
        "\n",
        "Are all variables usable for analysis and prediction of house prices? If the data contains missing values (or strange or nonsensical observations), can they be imputed (corrected), or must they be removed from the dataset?\n",
        "\n",
        "---\n",
        "\n",
        "### Question 3\n",
        "\n",
        "For the selected variables (`price`, `sqft_living`, `grade`, `yr_built`), verify whether the split into train, test, and validation datasets was random. That is, do these variables have approximately the same distributions across the train, test, and validation groups?\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "AMYcRDfIllU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import math"
      ],
      "metadata": {
        "id": "0SZFOo2WDobR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1\n"
      ],
      "metadata": {
        "id": "3COThmZyjosF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the dimensions of the dataset\n",
        "print(\"\\nDimensions of the dataset:\", house_rad.shape)\n",
        "\n",
        "# Data types of individual variables\n",
        "print(\"\\nData types of variables:\")\n",
        "print(house_rad.dtypes)\n",
        "\n",
        "# Summarize basic descriptive statistics of all variables\n",
        "print(\"\\nDescriptive statistics:\")\n",
        "print(house_rad.describe(include='all'))"
      ],
      "metadata": {
        "id": "bZ_MVRHbmmnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Potential Outliers:**\n",
        "\n",
        "  - Bathrooms: The maximum value (275) is abnormally high.\n",
        "  - Grade: The maximum value (232) is unusually high compared to the 75th percentile (8), suggesting extreme outliers.\n",
        "  - Sqft_living and Sqft_lot: The large maximum values (33,600 and 641,203, respectively) compared to their means and 75th percentiles suggest outliers.\n",
        "  - Sqft_living and Sqft_lot: Small minimum value (105 and 12) might be an outlier.\n",
        "\n",
        "**Missing or Sparse Data:**\n",
        "\n",
        "  - Waterfront: The mean value is 0.021, indicating that only about 2.1% of houses have a waterfront view.\n",
        "  - Yr_renovated: Most houses likely have not been renovated, as the median value is 0, which might represent missing or unrenovated properties.\n",
        "  - There aren't any NAN values in the dataset\n",
        "\n",
        "**Notes and observations**\n",
        "\n",
        "  - Year Built: The range (1900-2015) suggests a mix of old and modern homes.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MAXanteWGqxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histograms of the specified columns\n",
        "house_rad.hist(bins=30, figsize=(20, 20), grid=False, edgecolor='black', alpha=0.7)\n",
        "plt.suptitle(\"Filtered Data\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HCpu_NWacA-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histogram and density estimate for the target variable price\n",
        "plt.figure(figsize=(12, 6))\n",
        "house_rad['price'].hist(bins=100, alpha=0.7, label='Histogram', density=True)\n",
        "house_rad['price'].plot(kind='kde', label='Density Estimate', color='red')\n",
        "plt.title('Distribution of Sale Price')\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('Frequency/Density')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rRXhgOfgELAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram suggest that the price distribution is right-skewed, with most prices concentrated in the lower range but a long tail extending to very high prices."
      ],
      "metadata": {
        "id": "2vDrjkTUEhlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Většina naměřených dat v hodnotách kolem milionu dolarů, zatímco pár měření výrazně přes milion - 4, 6, 7, přes 8."
      ],
      "metadata": {
        "id": "_naQq-hRdC-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2\n"
      ],
      "metadata": {
        "id": "hH0nB2xhjhVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in each column\n",
        "missing_values = house_rad.isnull().sum()\n",
        "\n",
        "# Display columns with missing values\n",
        "columns_with_nans = missing_values[missing_values > 0]\n",
        "\n",
        "print(\"Columns with missing values and their counts:\")\n",
        "print(columns_with_nans)\n"
      ],
      "metadata": {
        "id": "M7RCFj6vFniV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There aren't any variables with missing values"
      ],
      "metadata": {
        "id": "_dMlSFq4K0ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a subset of columns to visualize (price and continuous variables)\n",
        "columns_to_plot = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
        "                   'floors', 'view', 'sqft_above', 'sqft_basement',\n",
        "                   'yr_built', 'yr_renovated', 'sqft_living15', 'sqft_lot15']\n",
        "\n",
        "# Create pairplot\n",
        "sns.pairplot(house_rad[columns_to_plot], diag_kind='hist', height=2.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuiNDepEebJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot histograms for all columns\n",
        "# house_rad[columns_to_plot].hist(bins=30, figsize=(15, 10), grid=False, edgecolor='black', alpha=0.7)\n",
        "# plt.suptitle(\"All Data\", fontsize=16)\n",
        "# plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "-xjoEZ8UgUU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize potential outliers:"
      ],
      "metadata": {
        "id": "NXthtLdwdUS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_columns = house_rad.select_dtypes(include=['number']).columns[2:]\n",
        "num_plots = len(numeric_columns)\n",
        "\n",
        "# Define the grid size (e.g., 3 columns per row)\n",
        "ncols = 4\n",
        "nrows = math.ceil(num_plots / ncols)\n",
        "\n",
        "# Create a figure and a set of subplots\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 6, nrows * 5))\n",
        "axes = axes.flatten()  # Flatten in case of multiple rows\n",
        "\n",
        "# Loop through each numeric column and plot a boxplot\n",
        "for ax, col in zip(axes, numeric_columns):\n",
        "    sns.boxplot(x=house_rad[col], ax=ax)\n",
        "    ax.set_title(f\"Boxplot of {col}\", fontsize=14)\n",
        "    ax.set_xlabel(col, fontsize=12)\n",
        "    ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Remove any unused subplots\n",
        "for ax in axes[num_plots:]:\n",
        "    fig.delaxes(ax)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the grid of boxplots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OdEVyYyEfsdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure with 1 row and 2 columns of subplots\n",
        "fig, axs = plt.subplots(1, 2, figsize=(20, 6))  # Adjust figsize as needed\n",
        "\n",
        "# First Scatter Plot: Price vs. Living Area\n",
        "axs[0].scatter(house_rad['price'], house_rad['sqft_living'], alpha=0.5, color='blue')\n",
        "axs[0].set_title(\"Scatter Plot: Price vs. Living Area\", fontsize=16)\n",
        "axs[0].set_xlabel(\"Price\", fontsize=12)\n",
        "axs[0].set_ylabel(\"Living Area (sqft)\", fontsize=12)\n",
        "axs[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Second Scatter Plot: Price vs. Lot Area\n",
        "axs[1].scatter(house_rad['price'], house_rad['sqft_lot'], alpha=0.5, color='blue')\n",
        "axs[1].set_title(\"Scatter Plot: Price vs. Lot Area\", fontsize=16)\n",
        "axs[1].set_xlabel(\"Price\", fontsize=12)\n",
        "axs[1].set_ylabel(\"Lot Area (sqft)\", fontsize=12)\n",
        "axs[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gc8xR-kJdTm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zdá se, že hodnoty s vysokou cenou mohou být zatíženy chybou (panem Francem), neboť se nezdá že by reflektovaly trend a pro obrovské ceny mají velmi malou Living Area, stejně jako poměrně nízké ceny v poměru s obrovksými Living Area"
      ],
      "metadata": {
        "id": "H_csVJUFdsj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Vzhledem k tomu, že nevíme jaké jsou správné hodnoty, navrhruji nevyhovující hodnoty vyhodit.\n",
        "Např jak zmíněno výše: posledních několik hodnot u price jsou nepoměrně velké k ostatním a nedávají smysl, stejně jako hodnoty s nesmyslně velkým Living Area za neadekvátní ceny.\n",
        "\n",
        "Proměnná grade ná zřejmě také outliera, neboť se vyskytuje jedna hodnota 232 která narušuje všechny ostatní statistiky\n",
        "\n",
        "Proměnná Waterfall view by mohla činit problémy neboť je u většiny 0, ale je to dalši typ kategorické proměnné a je možné pro ni dělat regresi zvlášť?"
      ],
      "metadata": {
        "id": "mN4v-CDTdxzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization for the data with a basic outlier elimination\n",
        "# Filter data to remove outliers\n",
        "\n",
        "# KATKA\n",
        "# filtered_house_data = house_rad[\n",
        "#     (house_rad['bathrooms'] < 5) &\n",
        "#     (house_rad['sqft_living'] < 30000) &\n",
        "#     (house_rad['sqft_lot'] < 200000) &\n",
        "#     (house_rad['sqft_living15'] < 30000) &\n",
        "#     (house_rad['sqft_lot15'] < 200000)\n",
        "# ]\n",
        "\n",
        "# FILIP:\n",
        "data = house_rad\n",
        "filtered_house_data = data[\n",
        "    (data['bathrooms'] < 5) &\n",
        "    (data['sqft_living'] < 10000) &\n",
        "    (data['sqft_lot'] < 30000) &\n",
        "    (data['sqft_living15'] < 10000) &\n",
        "    (data['sqft_lot15'] < 30000) &\n",
        "    (data['price'] < 4000000) &\n",
        "    (data['grade'] < 20)\n",
        "]\n",
        "\n",
        "# Plot histograms of the specified columns\n",
        "filtered_house_data[columns_to_plot].hist(bins=30, figsize=(20, 20), grid=False, edgecolor='black', alpha=0.7)\n",
        "plt.suptitle(\"Filtered Data\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to fit title\n",
        "plt.show()\n",
        "\n",
        "summary_stats = filtered_house_data.describe(include='all')\n",
        "summary_stats"
      ],
      "metadata": {
        "id": "eUCiIH95IcYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Odebral jsem některé hodnoty podle plotů jednotlivých proměnných a podle histogramů. Postupoval jsem podle nějakých logických úsudků - living area větší než 10000 ft^2 neboli 929 m^2 nedává smysl, tak jsem to tímto způsobem omezil. Stejně jako cena nemovitosti - v scatter plotu price ku living space jsou byty s velmi malými living space řádově o několik milionu dražší, což nedává z logiky věci smysl, a proto jsem ji omezil (je otázka jesli mohu omezit cenu když je to naše target proměnná). Obdobně grade dává smysl 0-10, vyskytuje se ale i hodnota 13. Lot area je také - při velmi nízkých cenách prodeje jsou velmi obrovské pozemky, v řádu 2800 m^2."
      ],
      "metadata": {
        "id": "9TZZnMpEeO0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'filtered_house_data' is your DataFrame containing numeric columns\n",
        "numeric_columns = filtered_house_data.select_dtypes(include=['number']).columns[2:]\n",
        "num_plots = len(numeric_columns)\n",
        "\n",
        "# Define the grid size (e.g., 3 columns per row)\n",
        "ncols = 4\n",
        "nrows = math.ceil(num_plots / ncols)\n",
        "\n",
        "# Create a figure and a set of subplots\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 6, nrows * 5))\n",
        "axes = axes.flatten()  # Flatten in case of multiple rows\n",
        "\n",
        "# Loop through each numeric column and plot a boxplot\n",
        "for ax, col in zip(axes, numeric_columns):\n",
        "    sns.boxplot(x=filtered_house_data[col], ax=ax)\n",
        "    ax.set_title(f\"Boxplot of {col}\", fontsize=14)\n",
        "    ax.set_xlabel(col, fontsize=12)\n",
        "    ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Remove any unused subplots\n",
        "for ax in axes[num_plots:]:\n",
        "    fig.delaxes(ax)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the grid of boxplots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xxYoQxJEen31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3"
      ],
      "metadata": {
        "id": "5Ou7W1GI02TU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FIRST LOOK AT THE DATA BASED ON SPLIT"
      ],
      "metadata": {
        "id": "Fc7Qab0cg5ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figsize = (12, 1.2 * len(house_rad['split'].unique()))\n",
        "plt.figure(figsize=figsize)\n",
        "sns.violinplot(house_rad, x='price', y='split', inner='box', color=\"orange\")\n",
        "sns.despine(top=True, right=True, bottom=True, left=True)"
      ],
      "metadata": {
        "id": "FBbqdOqrLtOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figsize = (12, 1.2 * len(house_rad['split'].unique()))\n",
        "plt.figure(figsize=figsize)\n",
        "sns.violinplot(house_rad, x='sqft_living', y='split', inner='box', color='green')\n",
        "sns.despine(top=True, right=True, bottom=True, left=True)"
      ],
      "metadata": {
        "id": "HBKnOOJOL8Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figsize = (12, 1.2 * len(house_rad['split'].unique()))\n",
        "plt.figure(figsize=figsize)\n",
        "sns.violinplot(house_rad, x='grade', y='split', inner='box', color='grey')\n",
        "sns.despine(top=True, right=True, bottom=True, left=True)"
      ],
      "metadata": {
        "id": "F-ZsctuML9Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figsize = (12, 1.2 * len(house_rad['split'].unique()))\n",
        "plt.figure(figsize=figsize)\n",
        "sns.violinplot(house_rad, x='yr_built', y='split', inner='box', color=\"cyan\")\n",
        "sns.despine(top=True, right=True, bottom=True, left=True)"
      ],
      "metadata": {
        "id": "Z-VnATOJL9su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DENSITY ESTIMATES"
      ],
      "metadata": {
        "id": "DgjELNi9g-Gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_variables = ['price', 'sqft_living', 'grade', 'yr_built']\n",
        "\n",
        "# Create a 2x2 grid of subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))  # Adjust figsize as needed\n",
        "axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
        "\n",
        "# Loop through each variable and corresponding subplot axis\n",
        "for ax, variable in zip(axes, selected_variables):\n",
        "    sns.kdeplot(\n",
        "        data=data,\n",
        "        x=variable,\n",
        "        hue='split',\n",
        "        fill=True,\n",
        "        common_norm=False,\n",
        "        alpha=0.5,\n",
        "        ax=ax\n",
        "    )\n",
        "    # Set plot title and labels\n",
        "    ax.set_title(f\"Distribution of {variable.replace('_', ' ').title()} Across Splits\", fontsize=14)\n",
        "    ax.set_xlabel(variable.replace('_', ' ').title(), fontsize=12)\n",
        "    ax.set_ylabel(\"Density\", fontsize=12)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the grid of density plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AfjwNK6yg02m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visually, we can see there might be a problem with variable grade."
      ],
      "metadata": {
        "id": "WLkpI9f9Mhvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ks_2samp, f_oneway, kruskal\n",
        "\n",
        "# Group by split\n",
        "train_data = data[data['split'] == 'train']\n",
        "test_data = data[data['split'] == 'test']\n",
        "validation_data = data[data['split'] == 'validation']\n",
        "\n",
        "for variable in selected_variables:\n",
        "    print(f\"\\n=== Analysis for {variable} ===\")\n",
        "\n",
        "    # Kolmogorov-Smirnov Test: Train vs Test\n",
        "    ks_train_test = ks_2samp(train_data[variable], test_data[variable])\n",
        "    print(f\"KS Test (Train vs Test): Statistic={ks_train_test.statistic:.4f}, p-value={ks_train_test.pvalue:.4f}\")\n",
        "\n",
        "    # Kolmogorov-Smirnov Test: Train vs Validation\n",
        "    ks_train_validation = ks_2samp(train_data[variable], validation_data[variable])\n",
        "    print(f\"KS Test (Train vs Validation): Statistic={ks_train_validation.statistic:.4f}, p-value={ks_train_validation.pvalue:.4f}\")\n",
        "\n",
        "    # Kolmogorov-Smirnov Test: Test vs Validation\n",
        "    ks_test_validation = ks_2samp(test_data[variable], validation_data[variable])\n",
        "    print(f\"KS Test (Test vs Validation): Statistic={ks_test_validation.statistic:.4f}, p-value={ks_test_validation.pvalue:.4f}\")\n",
        "\n",
        "    # ANOVA\n",
        "    anova_result = f_oneway(train_data[variable], test_data[variable], validation_data[variable])\n",
        "    print(f\"ANOVA: F-statistic={anova_result.statistic:.4f}, p-value={anova_result.pvalue:.4f}\")\n",
        "\n",
        "    # Kruskal-Wallis Test\n",
        "    kruskal_result = kruskal(train_data[variable], test_data[variable], validation_data[variable])\n",
        "    print(f\"Kruskal-Wallis Test: H-statistic={kruskal_result.statistic:.4f}, p-value={kruskal_result.pvalue:.4f}\")"
      ],
      "metadata": {
        "id": "ZcsT4x-EhxaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pouze u analýzy **'grade'** nám p-value u Anovy vyšla velmi nízká, ale i tak ne menší než 0.05, takže bychom hypotézu, že jsou střední hodnoty tří rozdělení odlišné nezamítly. I když na obrázku PDFs je vidět, že split u grade není tak podobný jako u jiných proměnných\n"
      ],
      "metadata": {
        "id": "avGOrkDWh2e9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Linear Model (Use Only Training Data, i.e., split == \"train\")\n",
        "\n",
        "### Question 4\n",
        "\n",
        "Calculate the correlations between the regressors and visualize them. Also, compute the condition number (Kappa) and the variance inflation factor (VIF). If multicollinearity is present, decide which variables to use and justify your choices.\n",
        "\n",
        "---\n",
        "\n",
        "### Question 5\n",
        "\n",
        "Using only the training data (split == \"train\") and all selected variables, find a suitable linear regression model that best predicts the target variable `price`, i.e., minimizes the mean squared error (MSE). Compare the VIF and Kappa values of the final model to those of the original regressor matrix.\n",
        "\n",
        "---\n",
        "\n",
        "### Question 6\n",
        "\n",
        "For your selected model from the previous question, calculate the relevant influence measures. Provide the `id` for the top 20 observations with the highest values of DIFF, the highest leverage (hat values), and the highest Cook’s distance (i.e., 3 sets of 20 values). Which observations do you consider influential or outliers?\n",
        "\n",
        "---\n",
        "\n",
        "### Question 7\n",
        "\n",
        "Validate the model graphically using residual plots (Residual vs. Fitted, QQ-plot, Cook’s distance, leverage, etc.). Based on this and the previous question, did you identify any suspicious observations in the data that might have resulted from dataset adjustments? Would you recommend removing these observations from the data?\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "vmyvXCcamkp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4"
      ],
      "metadata": {
        "id": "3FBnjeHSLWVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only numeric columns\n",
        "numeric_columns = house_rad.select_dtypes(include=['number'])\n",
        "\n",
        "# Compute the correlation matrix\n",
        "correlation_matrix = numeric_columns.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "uo9hayaCgpDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation:\n",
        "- price and sqft_living, sqft_above, sft_living14\n",
        "- price view, waterfront, floors, bedrooms, yr_renovated\n",
        "- svarious sqft correlations (as expected)\n",
        "\n",
        "Negative correlation:\n",
        "- year build vs year renovated (as expected)"
      ],
      "metadata": {
        "id": "SrrY-0_2jwfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5\n"
      ],
      "metadata": {
        "id": "bldAcyweksCE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BAFdnVAVmgXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_Q5(data, selected_features, split):\n",
        "    print(\"Model for data split:\", split)\n",
        "    split_data = data[data['split'] == split]\n",
        "    X = split_data[selected_features]\n",
        "    y = split_data['price']\n",
        "\n",
        "    # Handle missing values\n",
        "    X = X.fillna(X.mean())\n",
        "    y = y.fillna(y.mean())\n",
        "\n",
        "    # # Create interaction terms\n",
        "    # X['sqft_living_grade'] = X['sqft_living'] * X['grade']\n",
        "    # X['bathrooms_bedrooms'] = X['bathrooms'] * X['bedrooms']\n",
        "\n",
        "    # Add a constant for statsmodels regression\n",
        "    X_with_const = sm.add_constant(X)\n",
        "\n",
        "    # Fit the regression model\n",
        "    model = sm.OLS(y, X_with_const).fit()\n",
        "\n",
        "    # Print the summary\n",
        "    print(model.summary())\n",
        "\n",
        "    # Calculate Mean Squared Error (MSE)\n",
        "    y_pred = model.predict(X_with_const)\n",
        "    mse = ((y - y_pred) ** 2).mean()\n",
        "\n",
        "    # Calculate kappa (condition number of the design matrix)\n",
        "    kappa = np.linalg.cond(X_with_const)\n",
        "\n",
        "    # Calculate VIF for each feature\n",
        "    vif_data = pd.DataFrame({\n",
        "        \"Variable\": X_with_const.columns,\n",
        "        \"VIF\": [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]\n",
        "    })\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\nVariance Inflation Factors (VIF):\")\n",
        "    print(vif_data)\n",
        "    print(f\"\\nMean Squared Error (MSE): {mse}\")\n",
        "    print(f\"Kappa (Condition Number): {kappa}\")\n",
        "    print('\\n')\n"
      ],
      "metadata": {
        "id": "PlWHR2aDw-yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
        "                     'floors', 'waterfront', 'view', 'condition',\n",
        "                     'grade', 'sqft_above', 'sqft_basement',\n",
        "                     'yr_built', 'yr_renovated', 'sqft_living15', 'sqft_lot15']\n",
        "selected_features = ['sqft_living',\n",
        "                     'floors', 'view',\n",
        "                     'sqft_basement',\n",
        "                     'yr_renovated']\n",
        "\n",
        "\n",
        "model_Q5(house_rad, selected_features, split='train')"
      ],
      "metadata": {
        "id": "69VHJntQrlJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Robust Approach\n",
        "\n",
        "### Question 8\n",
        "\n",
        "If you decided to remove any observations from the dataset, work with the filtered dataset, retrain the model from Question 5, and calculate the $R^2$ statistic and MSE on both the training and test data (split == \"test\").\n",
        "\n",
        "---\n",
        "\n",
        "### Question 9\n",
        "\n",
        "Estimate the regression coefficients using a robust method, such as Total Least Squares (TLS), on both the filtered dataset (after removing any observations, if applicable) and the original full dataset. Compare the results and discuss any differences in the estimated coefficients and model performance. What insights can you draw about the impact of filtering observations on model robustness?\n",
        "\n",
        "---\n",
        "### Question 10\n",
        "\n",
        "Explore additional robust regression approaches (e.g., Huber regression, quantile regression, or M-estimators) to estimate the regression coefficients on the filtered and full datasets. Compare the results across these methods, discussing the strengths and limitations of each approach in the context of predicting house prices in the King County area. How do these robust methods handle potential outliers or influential observations in the data?\n",
        "\n",
        "---\n",
        "### Question 11\n",
        "\n",
        "Select the final model and compare the MSE and $R^2$ on the training, test, and validation datasets. What do these values suggest about the quality of the model and potential overfitting? Is your model suitable for predicting house prices in the King County area? If so, does this prediction have any limitations?\n",
        "\n"
      ],
      "metadata": {
        "id": "OGfHy5Gwrkp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8"
      ],
      "metadata": {
        "id": "lckVjC3NoemB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display rows with extreme values for bathrooms\n",
        "\n",
        "\n",
        "outliers = house_rad[house_rad['bathrooms'] > 5]\n",
        "print(outliers.shape)\n",
        "\n",
        "outliers = house_rad[house_rad['sqft_living'] < house_rad['sqft_above']]\n",
        "print(outliers.shape)\n",
        "\n",
        "outliers = house_rad[house_rad['sqft_living'] < house_rad['sqft_basement']]\n",
        "print(outliers.shape)\n",
        "\n",
        "outliers = house_rad[house_rad['sqft_lot'] < house_rad['sqft_above']]\n",
        "print(outliers.shape)\n",
        "\n",
        "outliers = house_rad[house_rad['sqft_lot'] < house_rad['sqft_basement']]\n",
        "print(outliers.shape)\n",
        "\n",
        "# outliers = house_rad[house_rad['sqft_lot'] < house_rad['sqft_living']]  # doesn't necesarily logically need to be an outlier - building can be vertical\n",
        "# print(outliers.shape)\n",
        "\n",
        "# outliers = house_rad[house_rad['sqft_lot15'] < house_rad['sqft_living15']]\n",
        "# print(outliers.shape)\n",
        "\n",
        "outliers = house_rad[house_rad['sqft_lot'] < 100]\n",
        "print(outliers.shape)\n",
        "\n",
        "outliers = house_rad[(house_rad['sqft_living'] < 500) & (house_rad['bathrooms'] > 2)]  # not added condition, just seemed weird\n",
        "print(outliers.shape)\n",
        "\n",
        "# Define conditions\n",
        "conditions = (\n",
        "    (house_rad['bathrooms'] <= 5)\n",
        "    & (house_rad['sqft_lot'] > 100)\n",
        "    & (house_rad['sqft_living'] >= house_rad['sqft_above'])\n",
        "    & (house_rad['sqft_living'] >= house_rad['sqft_basement'])\n",
        "    & (house_rad['sqft_lot'] >= house_rad['sqft_above'])\n",
        "    & (house_rad['sqft_lot'] >= house_rad['sqft_basement'])\n",
        "    # & (house_rad['sqft_lot'] >= house_rad['sqft_living'])\n",
        "    # & (house_rad['sqft_lot15'] >= house_rad['sqft_living15'])\n",
        ")\n",
        "\n",
        "# Apply the conditions to filter the dataset\n",
        "filtered_data = house_rad[conditions]\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Original dataset size: {house_rad.shape}\")\n",
        "print(f\"Filtered dataset size after manual removal: {filtered_data.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Vajj99RbrW0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test on model from question 5\n",
        "model_Q5(filtered_data, selected_features, split=\"train\")\n",
        "model_Q5(filtered_data, selected_features, split=\"test\")"
      ],
      "metadata": {
        "id": "SFCM-Qwiwvzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5kse02wVrWfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9\n"
      ],
      "metadata": {
        "id": "vyzBQAL7zEhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ON UNFILTERED DATA\n",
        "\n",
        "X = house_rad[selected_features]\n",
        "y = house_rad['price']\n",
        "\n",
        "X_with_const = sm.add_constant(X) # Add constant for intercept\n",
        "\n",
        "# Fit Robust Linear Model\n",
        "rlm_model_all = sm.RLM(y, X_with_const)\n",
        "rlm_results_all = rlm_model_all.fit()\n",
        "print(rlm_results_all.summary())"
      ],
      "metadata": {
        "id": "bhI_lJGozJD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ON FILTERED DATA\n",
        "\n",
        "X = filtered_data[selected_features]\n",
        "y = filtered_data['price']\n",
        "\n",
        "X_with_const = sm.add_constant(X) # Add constant for intercept\n",
        "\n",
        "# Fit Robust Linear Model\n",
        "rlm_model_filtered = sm.RLM(y, X_with_const)\n",
        "rlm_results_filtered = rlm_model_filtered.fit()\n",
        "print(rlm_results_filtered.summary())"
      ],
      "metadata": {
        "id": "AOCe1dKx0Pir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*TODO:\n",
        "Compare the results and discuss any differences in the estimated coefficients and model performance. What insights can you draw about the impact of filtering observations on model robustness?*"
      ],
      "metadata": {
        "id": "ZYwFcGog0ZuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10\n"
      ],
      "metadata": {
        "id": "0iXBGOk5DUNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "2E82OREuFP55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def huber_model(data):\n",
        "    # Prepare data\n",
        "    X = data[selected_features]\n",
        "    y = data['price']\n",
        "\n",
        "    # Fit Huber Regressor\n",
        "    huber = HuberRegressor()\n",
        "    huber.fit(X, y)\n",
        "\n",
        "    # Get coefficients and intercept\n",
        "    coefficients = huber.coef_\n",
        "    intercept = huber.intercept_\n",
        "\n",
        "    # Predict and evaluate\n",
        "    y_pred = huber.predict(X)\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "\n",
        "    print(\"Coefficients:\", coefficients)\n",
        "    print(\"Intercept:\", intercept)\n",
        "    print(\"MSE:\", mse)\n",
        "\n",
        "print(\"All data\")\n",
        "huber_model(house_rad)\n",
        "\n",
        "print('\\n')\n",
        "print(\"Filtered data\")\n",
        "huber_model(filtered_data)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "28M80gXkDYCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quantile_model(data, q=0.5):\n",
        "    # Prepare data\n",
        "    X = data[selected_features].fillna(0)\n",
        "    y = data['price'].fillna(0)\n",
        "\n",
        "    # Add constant for intercept\n",
        "    X_with_const = sm.add_constant(X)\n",
        "\n",
        "    # Fit Quantile Regression\n",
        "    quantile_reg = sm.QuantReg(y, X_with_const)\n",
        "    quantile_results = quantile_reg.fit(q=q)\n",
        "\n",
        "    print(quantile_results.summary())\n",
        "\n",
        "    # Predict and evaluate\n",
        "    y_pred = quantile_results.predict(X_with_const)\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "\n",
        "    print(f\"Quantile (q={q}) Regression Results\")\n",
        "    print(\"Coefficients:\", quantile_results.params.values)\n",
        "    print(\"Intercept:\", quantile_results.params['const'])\n",
        "    print(\"MSE:\", mse)\n",
        "\n",
        "print(\"All data\")\n",
        "quantile_model(house_rad, q=0.5)\n",
        "\n",
        "print('\\n')\n",
        "print(\"Filtered data\")\n",
        "quantile_model(filtered_data, q=0.5)\n"
      ],
      "metadata": {
        "id": "4KkD0S14Dt3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def m_estimator_model(data):\n",
        "    # Prepare data\n",
        "    X = data[selected_features].fillna(0)\n",
        "    y = data['price'].fillna(0)\n",
        "\n",
        "    # Add constant for intercept\n",
        "    X_with_const = sm.add_constant(X)\n",
        "\n",
        "    # Fit Robust Linear Model (M-Estimator)\n",
        "    m_estimator = sm.RLM(y, X_with_const)\n",
        "    m_estimator_results = m_estimator.fit()\n",
        "\n",
        "    print(m_estimator_results.summary())\n",
        "\n",
        "    # Predict and evaluate\n",
        "    y_pred = m_estimator_results.predict(X_with_const)\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "\n",
        "    print(\"M-Estimator Regression Results\")\n",
        "    print(\"Coefficients:\", m_estimator_results.params.values)\n",
        "    print(\"Intercept:\", m_estimator_results.params['const'])\n",
        "    print(\"MSE:\", mse)\n",
        "\n",
        "print(\"All data\")\n",
        "m_estimator_model(house_rad)\n",
        "\n",
        "print('\\n')\n",
        "print(\"Filtered data\")\n",
        "m_estimator_model(filtered_data)\n"
      ],
      "metadata": {
        "id": "wwb7bQlSF9OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*TODO Compare the results across these methods, discussing the strengths and limitations of each approach in the context of predicting house prices in the King County area. How do these robust methods handle potential outliers or influential observations in the data?*"
      ],
      "metadata": {
        "id": "_zZweZPFGcq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 11\n"
      ],
      "metadata": {
        "id": "NEmCd7shGp-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Voluntary part: Machine Learning Approach\n",
        "\n",
        "\n",
        "### Question 12\n",
        "Apply machine learning-based linear regression methods, such as Ridge Regression, Lasso, or Elastic Net, to the dataset. Use the train-test split to evaluate model performance and focus on feature selection. Identify the most relevant features based on these methods and compare how the selected features impact the model's predictive performance. Discuss how regularization affects feature selection and the trade-offs between bias and variance in the context of house price prediction. Additionally, evaluate the stability of selected features across different methods and provide recommendations for choosing the optimal feature set.\n"
      ],
      "metadata": {
        "id": "ySe_Y1iFqwzk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "acHsuw_tGsmO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}