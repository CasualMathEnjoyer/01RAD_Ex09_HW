{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CasualMathEnjoyer/01RAD_Ex09_HW/blob/main/ex09_hw_log_transform%2Bscaler_pro_kappa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01RAD  Exercise 09 - Homework\n",
        "\n",
        "Authors: Your names here  \n",
        "Date: 2024-12-03  \n",
        "\n",
        "---\n",
        "\n",
        "## Task Description\n",
        "\n",
        "The dataset is based on the **House Sales in King County, USA** dataset, which can be found, for example, on kaggle.com or in the `moderndive` library under the name `house_prices`. The original dataset contains house sale prices in the King County area, which includes Seattle, and the data was collected between May 2014 and May 2015. For our purposes, several variables have been removed, and the dataset has been significantly reduced and slightly modified.\n",
        "\n",
        "The dataset has already been split into three parts and modified, all of which will be used progressively throughout this assignment.\n",
        "\n",
        "---\n",
        "\n",
        "## Variables Description\n",
        "\n",
        "The dataset contains the following 18 variables, and our goal is to explore the influence of 12 of them on the target variable `price`.\n",
        "\n",
        "| Feature         | Description                                           |\n",
        "|------------------|-------------------------------------------------------|\n",
        "| `id`            | Unique identifier for a house                         |\n",
        "| `price`         | Sale price (prediction target)                        |\n",
        "| `bedrooms`      | Number of bedrooms                                    |\n",
        "| `bathrooms`     | Number of bathrooms                                   |\n",
        "| `sqft_living`   | Square footage of the home                            |\n",
        "| `sqft_lot`      | Square footage of the lot                             |\n",
        "| `floors`        | Total number of floors (levels) in the house          |\n",
        "| `waterfront`    | Whether the house has a waterfront view               |\n",
        "| `view`          | Number of times the house has been viewed             |\n",
        "| `condition`     | Overall condition of the house                        |\n",
        "| `grade`         | Overall grade given to the housing unit               |\n",
        "| `sqft_above`    | Square footage of the house apart from the basement   |\n",
        "| `sqft_basement` | Square footage of the basement                        |\n",
        "| `yr_built`      | Year the house was built                              |\n",
        "| `yr_renovated`  | Year when the house was renovated                     |\n",
        "| `sqft_living15` | Living room area in 2015 (after renovations)          |\n",
        "| `sqft_lot15`    | Lot size in 2015 (after renovations)                  |\n",
        "| `split`         | Splitting variable with train, test, and validation samples |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "WQJA5t5UlK9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import math"
      ],
      "metadata": {
        "id": "FzTItGOdBa9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/francji1/01RAD/main/data/01RAD_2024_house.csv\"\n",
        "house_rad = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "house_rad.head()\n"
      ],
      "metadata": {
        "id": "b4IQE-BBl5S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Exploratory and Graphical Analysis\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AMYcRDfIllU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1\n",
        "\n",
        "Verify the dimensions of the dataset, the types of individual variables, and summarize the basic descriptive statistics of all variables. Plot a histogram and a density estimate for the target variable `price`. Can anything be inferred for future analysis?"
      ],
      "metadata": {
        "id": "KyDgh3hrZXAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the dimensions of the dataset\n",
        "print(\"\\nDimensions of the dataset:\", house_rad.shape)\n",
        "\n",
        "# Data types of individual variables\n",
        "print(\"\\nData types of variables:\")\n",
        "print(house_rad.dtypes)\n",
        "\n",
        "# Summarize basic descriptive statistics of all variables\n",
        "print(\"\\nDescriptive statistics:\")\n",
        "print(house_rad.describe(include='all'))"
      ],
      "metadata": {
        "id": "yrGbjER1B3Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histogram and density estimate for the target variable price\n",
        "plt.figure(figsize=(12, 6))\n",
        "house_rad['price'].hist(bins=100, alpha=0.7, label='Histogram', density=True)\n",
        "house_rad['price'].plot(kind='kde', label='Density Estimate', color='red')\n",
        "plt.title('Distribution of Sale Price')\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('Frequency/Density')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mwI1CKR9CE_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram suggest that the price distribution is right-skewed, with most prices concentrated in the lower range but a long tail extending to very high prices."
      ],
      "metadata": {
        "id": "gIJq9Z7TCZHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "house_rad['price'].plot.box()"
      ],
      "metadata": {
        "id": "gfjafilwCcCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most values for price are around one milion dollars. Above four milion may be nonsensical."
      ],
      "metadata": {
        "id": "xvEhbm2yC9fW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### Question 2\n",
        "\n",
        "Are all variables usable for analysis and prediction of house prices? If the data contains missing values (or strange or nonsensical observations), can they be imputed (corrected), or must they be removed from the dataset?\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "KEL6IsA7BeoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## unusable variables ##\n",
        "\n",
        "Variables Unnamed: 0 and id may be dropped right away"
      ],
      "metadata": {
        "id": "0ArYU8vbDyK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "house_rad = house_rad.drop(columns=[\"Unnamed: 0\", \"id\"])"
      ],
      "metadata": {
        "id": "nMfolxs1D5fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## missing values ##"
      ],
      "metadata": {
        "id": "FsKaqmEZGLSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in each column\n",
        "missing_values = house_rad.isnull().sum()\n",
        "\n",
        "# Display columns with missing values\n",
        "columns_with_nans = missing_values[missing_values > 0]\n",
        "\n",
        "print(\"Columns with missing values and their counts:\")\n",
        "print(columns_with_nans)"
      ],
      "metadata": {
        "id": "vulTfLRwGO4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "no missing values"
      ],
      "metadata": {
        "id": "DEBSjZp6Gjsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nonsensical values ##"
      ],
      "metadata": {
        "id": "_9rA8uMxETjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_plots = len(house_rad.columns)\n",
        "\n",
        "# Define the grid size (e.g., 3 columns per row)\n",
        "ncols = 4\n",
        "nrows = math.ceil(num_plots / ncols)\n",
        "\n",
        "# Create a figure and a set of subplots\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 6, nrows * 5))\n",
        "axes = axes.flatten()  # Flatten in case of multiple rows\n",
        "\n",
        "# Loop through each numeric column and plot a boxplot\n",
        "for ax, col in zip(axes, house_rad):\n",
        "    sns.boxplot(x=house_rad[col], ax=ax)\n",
        "    ax.set_title(f\"Boxplot of {col}\", fontsize=14)\n",
        "    ax.set_xlabel(col, fontsize=12)\n",
        "    ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Remove any unused subplots\n",
        "for ax in axes[num_plots:]:\n",
        "    fig.delaxes(ax)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the grid of boxplots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UD52a5zZHZo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = house_rad[\n",
        "    (house_rad['bathrooms'] < 5) &\n",
        "    (house_rad['sqft_living'] > 400) &\n",
        "    (house_rad['sqft_living15'] > 400) &\n",
        "    (house_rad['sqft_lot'] > 400) &\n",
        "    (house_rad['sqft_lot15'] > 400) &\n",
        "    (house_rad['grade'] < 15)\n",
        "]"
      ],
      "metadata": {
        "id": "KG5vUEpCEa5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "mzvjszD-FzOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We remove nonensical values as the dataset has sufficient size"
      ],
      "metadata": {
        "id": "O_mIxOacGq25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histograms of the specified columns\n",
        "data.hist(bins=30, figsize=(20, 20), grid=False, edgecolor='black', alpha=0.7)\n",
        "plt.suptitle(\"Filtered Data\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VHXsPaRyG5by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(data, diag_kind='hist', height=2.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KRrwA-w0HMc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zdá se, že hodnoty s vysokou cenou mohou být zatíženy chybou (panem Francem), neboť se nezdá že by reflektovaly trend a pro obrovské ceny mají velmi malou Living Area, stejně jako poměrně nízké ceny v poměru s obrovksými Living Area\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Vzhledem k tomu, že nevíme jaké jsou správné hodnoty, navrhruji nevyhovující hodnoty vyhodit.\n",
        "Např jak zmíněno výše: posledních několik hodnot u price jsou nepoměrně velké k ostatním a nedávají smysl, stejně jako hodnoty s nesmyslně velkým Living Area za neadekvátní ceny.\n",
        "\n",
        "Proměnná grade ná zřejmě také outliera, neboť se vyskytuje jedna hodnota 232 která narušuje všechny ostatní statistiky\n",
        "\n",
        "Proměnná Waterfall view by mohla činit problémy neboť je u většiny 0"
      ],
      "metadata": {
        "id": "LoREx9QxH2ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Question 3\n",
        "\n",
        "For the selected variables (`price`, `sqft_living`, `grade`, `yr_built`), verify whether the split into train, test, and validation datasets was random. That is, do these variables have approximately the same distributions across the train, test, and validation groups?\n"
      ],
      "metadata": {
        "id": "C89phTqlBe2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figsize = (12, 1.2 * len(data['split'].unique()))\n",
        "plt.figure(figsize=figsize)\n",
        "sns.violinplot(data, x='price', y='split', inner='box', color=\"orange\")\n",
        "sns.despine(top=True, right=True, bottom=True, left=True)\n",
        "plt.title('price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2lTnOq10IPGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "figsize = (12, 1.2 * len(data['split'].unique()))\n",
        "plt.figure(figsize=figsize)\n",
        "sns.violinplot(data, x='sqft_living', y='split', inner='box', color='green')\n",
        "sns.despine(top=True, right=True, bottom=True, left=True)\n",
        "plt.title('sqft_living')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bZ_MVRHbmmnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figsize = (12, 1.2 * len(data['split'].unique()))\n",
        "plt.figure(figsize=figsize)\n",
        "sns.violinplot(data, x='grade', y='split', inner='box', color='grey')\n",
        "sns.despine(top=True, right=True, bottom=True, left=True)\n",
        "plt.title('grade')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WwmVykk0IYe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figsize = (12, 1.2 * len(data['split'].unique()))\n",
        "plt.figure(figsize=figsize)\n",
        "sns.violinplot(data, x='yr_built', y='split', inner='box', color=\"cyan\")\n",
        "sns.despine(top=True, right=True, bottom=True, left=True)\n",
        "plt.title('yr_built')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DKmEVq3dIbZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_variables = ['price', 'sqft_living', 'grade', 'yr_built']\n",
        "\n",
        "# Create a 2x2 grid of subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))  # Adjust figsize as needed\n",
        "axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
        "\n",
        "# Loop through each variable and corresponding subplot axis\n",
        "for ax, variable in zip(axes, selected_variables):\n",
        "    sns.kdeplot(\n",
        "        data=data,\n",
        "        x=variable,\n",
        "        hue='split',\n",
        "        fill=True,\n",
        "        common_norm=False,\n",
        "        alpha=0.5,\n",
        "        ax=ax\n",
        "    )\n",
        "    # Set plot title and labels\n",
        "    ax.set_title(f\"Distribution of {variable.replace('_', ' ').title()} Across Splits\", fontsize=14)\n",
        "    ax.set_xlabel(variable.replace('_', ' ').title(), fontsize=12)\n",
        "    ax.set_ylabel(\"Density\", fontsize=12)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the grid of density plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xkKoa6m6IenY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by split\n",
        "train_data = data[data['split'] == 'train'].drop(columns=['split'])\n",
        "test_data = data[data['split'] == 'test'].drop(columns=['split'])\n",
        "validation_data = data[data['split'] == 'validation'].drop(columns=['split'])"
      ],
      "metadata": {
        "id": "tENQjvC3c2Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ks_2samp, f_oneway, kruskal\n",
        "\n",
        "for variable in selected_variables:\n",
        "    print(f\"\\n=== Analysis for {variable} ===\")\n",
        "\n",
        "    # Kolmogorov-Smirnov Test: Train vs Test\n",
        "    ks_train_test = ks_2samp(train_data[variable], test_data[variable])\n",
        "    print(f\"KS Test (Train vs Test): Statistic={ks_train_test.statistic:.4f}, p-value={ks_train_test.pvalue:.4f}\")\n",
        "\n",
        "    # Kolmogorov-Smirnov Test: Train vs Validation\n",
        "    ks_train_validation = ks_2samp(train_data[variable], validation_data[variable])\n",
        "    print(f\"KS Test (Train vs Validation): Statistic={ks_train_validation.statistic:.4f}, p-value={ks_train_validation.pvalue:.4f}\")\n",
        "\n",
        "    # Kolmogorov-Smirnov Test: Test vs Validation\n",
        "    ks_test_validation = ks_2samp(test_data[variable], validation_data[variable])\n",
        "    print(f\"KS Test (Test vs Validation): Statistic={ks_test_validation.statistic:.4f}, p-value={ks_test_validation.pvalue:.4f}\")\n",
        "\n",
        "    # ANOVA\n",
        "    anova_result = f_oneway(train_data[variable], test_data[variable], validation_data[variable])\n",
        "    print(f\"ANOVA: F-statistic={anova_result.statistic:.4f}, p-value={anova_result.pvalue:.4f}\")\n",
        "\n",
        "    # Kruskal-Wallis Test\n",
        "    kruskal_result = kruskal(train_data[variable], test_data[variable], validation_data[variable])\n",
        "    print(f\"Kruskal-Wallis Test: H-statistic={kruskal_result.statistic:.4f}, p-value={kruskal_result.pvalue:.4f}\")"
      ],
      "metadata": {
        "id": "XLN9flHgIg-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Žádná z p-values není signifikantní."
      ],
      "metadata": {
        "id": "dTa0FNPXIk6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Linear Model (Use Only Training Data, i.e., split == \"train\")\n"
      ],
      "metadata": {
        "id": "vmyvXCcamkp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 4\n",
        "\n",
        "Calculate the correlations between the regressors and visualize them. Also, compute the condition number (Kappa) and the variance inflation factor (VIF). If multicollinearity is present, decide which variables to use and justify your choices."
      ],
      "metadata": {
        "id": "Asb1OnjpdXP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = train_data.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AMrGGouMJQ3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import cond\n",
        "# 2. Condition Number (Kappa)\n",
        "condition_number = cond(train_data.values)\n",
        "print(f\"Condition Number (Kappa): {condition_number:.2f}\")\n",
        "\n",
        "# 3. Variance Inflation Factor (VIF)\n",
        "# Add a constant to X for intercept\n",
        "X_with_const = sm.add_constant(train_data)\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['Variable'] = train_data.columns\n",
        "vif_data['VIF'] = [variance_inflation_factor(X_with_const.values, i + 1) for i in range(len(train_data.columns))]\n",
        "print(\"\\nVariance Inflation Factor (VIF):\")\n",
        "print(vif_data)"
      ],
      "metadata": {
        "id": "YgTLv081KLlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaler pro koeficient KAPPA"
      ],
      "metadata": {
        "id": "m4aTvCFFreKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "# Standardize the features (but not the target variable)\n",
        "scaler = StandardScaler()\n",
        "new_train_standardized = scaler.fit_transform(train_data)\n",
        "\n",
        "# Add constant (intercept) to the standardized features\n",
        "X_with_const_standardized = add_constant(new_train_standardized)\n",
        "\n",
        "# Calculate the Condition Number (Kappa)\n",
        "condition_number_standardized = np.linalg.cond(X_with_const_standardized)\n",
        "print(f\"Condition Number (Kappa) after standardization: {condition_number_standardized}\")\n",
        "\n",
        "\n",
        "scaled_df = pd.DataFrame(new_train_standardized, columns=train_data.columns)"
      ],
      "metadata": {
        "id": "o-njsYYosVSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We remove columns sqft_lot15 a sqft_above"
      ],
      "metadata": {
        "id": "FOnhwCt0LKfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaner_data = data.drop(columns=['sqft_lot15', 'sqft_above'])"
      ],
      "metadata": {
        "id": "HRQmmlfVLWuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaner_data_num = cleaner_data.drop(columns=['split'])\n",
        "correlation_matrix = cleaner_data_num.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HhwYGjBuLsf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaler pro koeficient KAPPA"
      ],
      "metadata": {
        "id": "2pBbUkrasXTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the features (but not the target variable)\n",
        "scaler = StandardScaler()\n",
        "new_train_standardized = scaler.fit_transform(cleaner_data_num)\n",
        "\n",
        "# Add constant (intercept) to the standardized features\n",
        "X_with_const_standardized = add_constant(new_train_standardized)\n",
        "\n",
        "# Calculate the Condition Number (Kappa)\n",
        "condition_number_standardized = np.linalg.cond(X_with_const_standardized)\n",
        "print(f\"Condition Number (Kappa) after standardization: {condition_number_standardized}\")\n",
        "\n",
        "\n",
        "scaled_df = pd.DataFrame(new_train_standardized, columns=cleaner_data_num.columns)"
      ],
      "metadata": {
        "id": "tkLH1kKvsT7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. Condition Number (Kappa)\n",
        "condition_number = cond(cleaner_data_num.values)\n",
        "print(f\"Condition Number (Kappa): {condition_number:.2f}\")\n",
        "\n",
        "# 3. Variance Inflation Factor (VIF)\n",
        "# Add a constant to X for intercept\n",
        "X_with_const = sm.add_constant(cleaner_data_num)\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['Variable'] = cleaner_data_num.columns\n",
        "vif_data['VIF'] = [variance_inflation_factor(X_with_const.values, i + 1) for i in range(len(cleaner_data_num.columns))]\n",
        "print(\"\\nVariance Inflation Factor (VIF):\")\n",
        "print(vif_data)"
      ],
      "metadata": {
        "id": "rX8mEWTdL1uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## Question 5\n",
        "\n",
        "Using only the training data (split == \"train\") and all selected variables, find a suitable linear regression model that best predicts the target variable `price`, i.e., minimizes the mean squared error (MSE). Compare the VIF and Kappa values of the final model to those of the original regressor matrix.\n",
        "\n"
      ],
      "metadata": {
        "id": "VYpd5GzbI-EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def minimize_mse(data, feature_set, split, interactions=False):\n",
        "    split_data = data[data['split'] == split]\n",
        "\n",
        "    best_features = None\n",
        "    lowest_mse = float(\"inf\")\n",
        "    best_model = None\n",
        "\n",
        "    # Generate all possible subsets of features\n",
        "    for k in range(1, len(feature_set) + 1):  # Try all subset sizes\n",
        "        for subset in combinations(feature_set, k):\n",
        "            # Fit the model with the current subset of features\n",
        "            model = model_creation_smf(split_data, subset, interactions=interactions)\n",
        "\n",
        "            # Predict on the training data\n",
        "            y_hat = model.predict(split_data)\n",
        "            y = split_data['price']\n",
        "\n",
        "            # Calculate MSE manually\n",
        "            mse = np.mean((y_hat - y) ** 2)\n",
        "\n",
        "            # Update the best model if this one has a lower MSE\n",
        "            if mse < lowest_mse:\n",
        "                lowest_mse = mse\n",
        "                best_features = subset\n",
        "                best_model = model\n",
        "\n",
        "    print(lowest_mse)\n",
        "\n",
        "    return best_model, best_features, lowest_mse"
      ],
      "metadata": {
        "id": "h0jo03skMDNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_creation_smf(data, selected_features, interactions=False):\n",
        "    if interactions:\n",
        "        formula = \"price ~ \" + \"*\".join(selected_features)\n",
        "    else:\n",
        "        formula = \"price ~ \" + \"+\".join(selected_features)\n",
        "\n",
        "    model = smf.ols(formula, data=data).fit()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "euYneb6YMX5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
        "       'waterfront', 'view', 'condition', 'grade', 'sqft_basement', 'yr_built',\n",
        "       'yr_renovated', 'sqft_living15']"
      ],
      "metadata": {
        "id": "WpYTUqSaOiWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_set = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_basement', 'yr_built', 'yr_renovated', 'sqft_living15']\n",
        "# best_model, best_features, best_mse = minimize_mse(cleaner_data, feature_set, \"train\", interactions=False)"
      ],
      "metadata": {
        "id": "izdSvN-3OdkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "('bedrooms',\n",
        " 'bathrooms',\n",
        " 'sqft_living',\n",
        " 'sqft_lot',\n",
        " 'floors',\n",
        " 'waterfront',\n",
        " 'view',\n",
        " 'condition',\n",
        " 'grade',\n",
        " 'sqft_basement',\n",
        " 'yr_built',\n",
        " 'yr_renovated',\n",
        " 'sqft_living15')"
      ],
      "metadata": {
        "id": "i_CB407aXv5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        " ### MSE is minimiseed by a model with all variables"
      ],
      "metadata": {
        "id": "P4_5C7yXSHKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_basement', 'yr_built', 'yr_renovated', 'sqft_living15']\n",
        "best_model =  model_creation_smf(cleaner_data[cleaner_data['split'] == \"train\"], best_features)"
      ],
      "metadata": {
        "id": "YqEhpEPQSpym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_model.summary())"
      ],
      "metadata": {
        "id": "GL8UH0-Kh4ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logaritmická transformace"
      ],
      "metadata": {
        "id": "oXY0M58FsoTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# Log-transform the target variable 'price' and define the regression formula\n",
        "formula = \"np.log(price + 1) ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + waterfront + view + condition + grade + sqft_basement + yr_built + yr_renovated + sqft_living15\"\n",
        "\n",
        "# Fit the linear regression model with the transformed target variable\n",
        "model = smf.ols(formula=formula, data=data).fit()\n",
        "\n",
        "# Print the summary of the regression model\n",
        "print(model.summary())\n",
        "\n",
        "best_model =  model_creation_smf(cleaner_data[cleaner_data['split'] == \"train\"], best_features)\n"
      ],
      "metadata": {
        "id": "uWI1ql4St4ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several of the variables have high values of P and low walues of coefitient - we remove: *bedrooms, sqft_living, sqft_lot, yr_renovated*\n",
        "\n",
        "**Removal of variables**"
      ],
      "metadata": {
        "id": "7Urh1zmeg_mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaner_data = cleaner_data.drop(columns=['bedrooms', 'sqft_living', 'sqft_lot', 'yr_renovated'])"
      ],
      "metadata": {
        "id": "LNAT4D_EikJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_features = ['bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_basement', 'yr_built', 'sqft_living15']"
      ],
      "metadata": {
        "id": "t-hggGB0jlOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# Log-transform the target variable 'price' and define the regression formula\n",
        "formula = \"np.log(price + 1) ~ bathrooms + floors + waterfront + view + condition + grade + sqft_basement + yr_built + sqft_living15\"\n",
        "\n",
        "# Fit the linear regression model with the transformed target variable\n",
        "model = smf.ols(formula=formula, data=data).fit()\n",
        "\n",
        "# Print the summary of the regression model\n",
        "print(model.summary())\n",
        "\n",
        "best_model =  model_creation_smf(cleaner_data[cleaner_data['split'] == \"train\"], best_features)"
      ],
      "metadata": {
        "id": "F8Z81KJKg9S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VIF and Kappa and residuals"
      ],
      "metadata": {
        "id": "dWx56UWiTeZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "residuals"
      ],
      "metadata": {
        "id": "v-tmV18UmgmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import log\n",
        "\n",
        "cleaned_data = cleaner_data[cleaner_data['split'] == \"train\"]\n",
        "X = cleaned_data[best_features]\n",
        "# Add constant term for the intercept in the OLS model\n",
        "X_const = sm.add_constant(X)\n",
        "y_l = cleaned_data['price']\n",
        "\n",
        "# Check for zero or negative values in the target variable\n",
        "print(y_l[y_l <= 0])\n",
        "\n",
        "# Count zero or negative values\n",
        "print(f\"Number of zero or negative values: {(y_l <= 0).sum()}\")\n",
        "\n",
        "print(y_l.head())\n",
        "\n",
        "\n",
        "# Apply log transformation to target variable\n",
        "y_log = np.log(y_l + 1)  # Shift by 1 to avoid log(0)\n",
        "\n",
        "# Re-fit the model\n",
        "model_log = sm.OLS(y_log, X_const).fit()\n",
        "print(model_log.summary())\n",
        "\n",
        "# Check residuals again\n",
        "predictions_log = model_log.predict(X_const)\n",
        "residuals_log = y_log - predictions_log\n"
      ],
      "metadata": {
        "id": "UN2jIT9yssSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Mean Squared Error\n",
        "mse = mean_squared_error(y_log, predictions_log)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "\n",
        "# Residual Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=predictions_log, y=residuals_log, alpha=0.6)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.title(\"Residuals vs Predicted Values\")\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n",
        "\n",
        "# Histogram of Residuals\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(residuals_log, kde=True, bins=30, color='blue')\n",
        "plt.title(\"Histogram of Residuals\")\n",
        "plt.xlabel(\"Residuals\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "# QQ Plot\n",
        "sm.qqplot(residuals_log, line='45', fit=True)\n",
        "plt.title(\"QQ Plot of Residuals\")\n",
        "plt.show()\n",
        "\n",
        "# Check for Heteroscedasticity\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=predictions_log, y=abs(residuals_log), alpha=0.6)\n",
        "plt.title(\"Absolute Residuals vs Predicted Values (Heteroscedasticity Check)\")\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Absolute Residuals\")\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u0p1JWMDvoRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaner_data_num = cleaner_data.drop(columns=['split'])"
      ],
      "metadata": {
        "id": "FRtF9MfHnhqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the features (but not the target variable)\n",
        "scaler = StandardScaler()\n",
        "new_train_standardized = scaler.fit_transform(cleaner_data_num)\n",
        "\n",
        "# Add constant (intercept) to the standardized features\n",
        "X_with_const_standardized = add_constant(new_train_standardized)\n",
        "\n",
        "# Calculate the Condition Number (Kappa)\n",
        "condition_number_standardized = np.linalg.cond(X_with_const_standardized)\n",
        "print(f\"Condition Number (Kappa) after standardization: {condition_number_standardized}\")\n",
        "\n",
        "\n",
        "scaled_df = pd.DataFrame(new_train_standardized, columns=cleaner_data_num.columns)"
      ],
      "metadata": {
        "id": "yaAer9YQnfO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Variance Inflation Factor (VIF)\n",
        "# Add a constant to X for intercept\n",
        "X_with_const = sm.add_constant(cleaner_data_num)\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['Variable'] = cleaner_data_num.columns\n",
        "vif_data['VIF'] = [variance_inflation_factor(X_with_const.values, i + 1) for i in range(len(cleaner_data_num.columns))]\n",
        "print(\"\\nVariance Inflation Factor (VIF):\")\n",
        "print(vif_data)"
      ],
      "metadata": {
        "id": "oAuZYjVYiWDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Question 6\n",
        "\n",
        "For your selected model from the previous question, calculate the relevant influence measures. Provide the `id` for the top 20 observations with the highest values of DIFF, the highest leverage (hat values), and the highest Cook’s distance (i.e., 3 sets of 20 values). Which observations do you consider influential or outliers?\n",
        "\n"
      ],
      "metadata": {
        "id": "cKoENKftI-yT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume `model` is your fitted OLS model\n",
        "influence = model_log.get_influence()\n",
        "\n",
        "# Extract influence measures\n",
        "dfbetas = influence.dfbetas  # DIFF (standardized changes in coefficients)\n",
        "leverage = influence.hat_matrix_diag  # Leverage (hat values)\n",
        "cooks_d = influence.cooks_distance[0]  # Cook's distance\n",
        "\n",
        "# Create a DataFrame to store these values along with observation IDs\n",
        "influence_measures = pd.DataFrame({\n",
        "    'Observation': range(len(dfbetas)),  # Assuming the index matches row numbers\n",
        "    'Leverage': leverage,\n",
        "    'Cooks_D': cooks_d,\n",
        "    'DIFF': dfbetas.max(axis=1)  # Taking the largest DFBETA value for each observation\n",
        "})\n",
        "\n",
        "# Sort and select top 20 for each measure\n",
        "top_diff = influence_measures.nlargest(20, 'DIFF')\n",
        "top_leverage = influence_measures.nlargest(20, 'Leverage')\n",
        "top_cooks_d = influence_measures.nlargest(20, 'Cooks_D')\n",
        "\n",
        "\n",
        "# Display top 20 observations for each measure\n",
        "print(\"Top 20 Observations by DIFF:\")\n",
        "print(top_diff)\n",
        "\n",
        "print(\"\\nTop 20 Observations by Leverage:\")\n",
        "print(top_leverage)\n",
        "\n",
        "print(\"\\nTop 20 Observations by Cook's Distance:\")\n",
        "print(top_cooks_d)"
      ],
      "metadata": {
        "id": "EMK8Zn6JYapr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Question 7\n",
        "\n",
        "Validate the model graphically using residual plots (Residual vs. Fitted, QQ-plot, Cook’s distance, leverage, etc.). Based on this and the previous question, did you identify any suspicious observations in the data that might have resulted from dataset adjustments? Would you recommend removing these observations from the data?\n",
        "\n"
      ],
      "metadata": {
        "id": "f7pNIS8ZI-7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "y = cleaner_data[cleaner_data['split']=='train']['price']\n",
        "# Predictions and residuals\n",
        "predictions = best_model.predict( cleaner_data[cleaner_data['split']=='train'])\n",
        "residuals = y - predictions\n",
        "\n",
        "# Mean Squared Error\n",
        "mse = mean_squared_error(y, predictions)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "\n",
        "# Residual Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=predictions, y=residuals, alpha=0.6)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.title(\"Residuals vs Predicted Values\")\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n",
        "\n",
        "# Histogram of Residuals\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(residuals, kde=True, bins=30, color='blue')\n",
        "plt.title(\"Histogram of Residuals\")\n",
        "plt.xlabel(\"Residuals\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "# QQ Plot\n",
        "sm.qqplot(residuals, line='45', fit=True)\n",
        "plt.title(\"QQ Plot of Residuals\")\n",
        "plt.show()\n",
        "\n",
        "# Check for Heteroscedasticity\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=predictions, y=abs(residuals), alpha=0.6)\n",
        "plt.title(\"Absolute Residuals vs Predicted Values (Heteroscedasticity Check)\")\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Absolute Residuals\")\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "69VHJntQrlJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_influence_measures_with_data(model, data):\n",
        "    \"\"\"\n",
        "    Summarize influence measures, flag outliers, and include original data columns.\n",
        "\n",
        "    :param model: Fitted regression model object from statsmodels.\n",
        "    :param data: DataFrame used to fit the regression model.\n",
        "    :return: DataFrame summarizing influence measures, flagged outliers, and original data.\n",
        "    \"\"\"\n",
        "    influence = model.get_influence()\n",
        "\n",
        "    # Extract measures\n",
        "    leverage = influence.hat_matrix_diag\n",
        "    cooks_distance = influence.cooks_distance[0]\n",
        "    dffits = influence.dffits[0]\n",
        "    dfbetas = influence.dfbetas\n",
        "    cov_ratios = influence.cov_ratio\n",
        "\n",
        "    # Number of observations and predictors\n",
        "    n = int(model.nobs)\n",
        "    p = int(model.df_model) + 1  # Add 1 to include the intercept\n",
        "\n",
        "    # Rule of Thumb thresholds\n",
        "    leverage_threshold = 2 * (p) / n\n",
        "    cooks_distance_threshold = 4 / n\n",
        "    dffits_threshold = 2 * np.sqrt((p) / n)\n",
        "    dfbetas_threshold = 2 / np.sqrt(n)\n",
        "\n",
        "    # Summarize outliers based on thresholds\n",
        "    flagged = {\n",
        "        'High Leverage': leverage > leverage_threshold,\n",
        "        'High Cook\\'s Distance': cooks_distance > cooks_distance_threshold,\n",
        "        'High DFFITS': np.abs(dffits) > dffits_threshold,\n",
        "    }\n",
        "\n",
        "    # Flag observations with high DFBETAS for any predictor\n",
        "    for j in range(dfbetas.shape[1]):\n",
        "        flagged[f'High DFBETAS (Predictor {j})'] = np.abs(dfbetas[:, j]) > dfbetas_threshold\n",
        "\n",
        "    # Create summary DataFrame\n",
        "    summary_df = pd.DataFrame({\n",
        "        'Leverage': leverage,\n",
        "        'Cook\\'s Distance': cooks_distance,\n",
        "        'DFFITS': dffits,\n",
        "        'Covariance Ratio': cov_ratios\n",
        "    })\n",
        "\n",
        "    # Add flags for rule-of-thumb violations\n",
        "    for key, flag in flagged.items():\n",
        "        summary_df[key] = flag\n",
        "\n",
        "    # Combine summary DataFrame with original data\n",
        "    summary_with_data = pd.concat([data.reset_index(drop=True), summary_df], axis=1)\n",
        "\n",
        "    # Select rows where any flag is True\n",
        "    flag_columns = [col for col in summary_df.columns if 'High' in col]\n",
        "    flagged_observations = summary_with_data.loc[summary_df[flag_columns].any(axis=1)]\n",
        "\n",
        "    return summary_with_data, flagged_observations\n"
      ],
      "metadata": {
        "id": "pywcv4S9Ypa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function\n",
        "all_observations_with_im, flagged_observations = summarize_influence_measures_with_data(model_log, cleaned_data)\n",
        "\n",
        "# Display the flagged observations\n",
        "flagged_observations"
      ],
      "metadata": {
        "id": "N9zuFgt2YsP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of observations and predictors\n",
        "n = int(model_log.nobs)\n",
        "p = int(model_log.df_model) + 1  # Include intercept\n",
        "\n",
        "# Define thresholds\n",
        "leverage_threshold = 2 * p / n\n",
        "cooks_d_threshold = 4 / n\n",
        "dffits_threshold = 2 * np.sqrt(p / n)\n",
        "dfbetas_threshold = 2 / np.sqrt(n)\n",
        "\n",
        "\n",
        "summary_influence = influence.summary_frame()"
      ],
      "metadata": {
        "id": "Q4hgWmuAZ939"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify observations exceeding the threshold\n",
        "cooks_d = summary_influence['cooks_d']\n",
        "influential_obs = cooks_d > cooks_d_threshold\n",
        "influential_indices = summary_influence[influential_obs].index\n",
        "\n",
        "# Create the stem plot of Cook's Distance\n",
        "plt.figure(figsize=(12, 8))\n",
        "markerline, stemlines, baseline = plt.stem(\n",
        "    summary_influence.index, cooks_d, markerfmt=\",\", basefmt=\" \"\n",
        ")\n",
        "plt.axhline(y=cooks_d_threshold, color='red', linestyle='--', label=f'Threshold ({cooks_d_threshold:.4f})')\n",
        "plt.xlabel('Observation Index')\n",
        "plt.ylabel(\"Cook's Distance\")\n",
        "plt.title(\"Cook's Distance for Each Observation\")\n",
        "plt.legend()\n",
        "\n",
        "# Annotate influential observations\n",
        "for idx in influential_indices:\n",
        "    plt.annotate(\n",
        "        str(idx),\n",
        "        (idx, cooks_d[idx]),\n",
        "        textcoords=\"offset points\",\n",
        "        xytext=(0, 10),  # Offset label above the point\n",
        "        ha='center',\n",
        "        fontsize=9,\n",
        "        color='blue'\n",
        "    )\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H0h8Le5faJ6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Robust Approach\n",
        "\n"
      ],
      "metadata": {
        "id": "OGfHy5Gwrkp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8\n",
        "\n",
        "If you decided to remove any observations from the dataset, work with the filtered dataset, retrain the model from Question 5, and calculate the $R^2$ statistic and MSE on both the training and test data (split == \"test\")."
      ],
      "metadata": {
        "id": "AgjQAnPXVbSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "train_y = cleaner_data[cleaner_data['split']=='train']['price']\n",
        "train_y_hat = best_model.predict(cleaner_data[cleaner_data['split']=='train'])\n",
        "\n",
        "test_y = cleaner_data[cleaner_data['split']=='test']['price']\n",
        "test_y_hat = best_model.predict(cleaner_data[cleaner_data['split']=='test'])\n",
        "\n",
        "# Calculate R-squared\n",
        "r_squared_train = r2_score(train_y, train_y_hat)\n",
        "r_squared_test = r2_score(test_y, test_y_hat)\n",
        "\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse_train = mean_squared_error(train_y, train_y_hat)\n",
        "mse_test = mean_squared_error(test_y, test_y_hat)\n",
        "\n",
        "print(f\"train: mse {mse_train}, R2 {r_squared_train}\")\n",
        "print(f\"test: mse {mse_test}, R2 {r_squared_test}\")\n"
      ],
      "metadata": {
        "id": "yyFRveO8lysn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Question 9\n",
        "\n",
        "Estimate the regression coefficients using a robust method, such as Total Least Squares (TLS), on both the filtered dataset (after removing any observations, if applicable) and the original full dataset. Compare the results and discuss any differences in the estimated coefficients and model performance. What insights can you draw about the impact of filtering observations on model robustness?\n"
      ],
      "metadata": {
        "id": "BVCUwDeYb2XB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "def total_least_squares(X, y):\n",
        "    \"\"\"\n",
        "    Estimate regression coefficients using Total Least Squares.\n",
        "\n",
        "    Args:\n",
        "        X (numpy.ndarray): Independent variables matrix.\n",
        "        y (numpy.ndarray): Dependent variable vector.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Estimated regression coefficients.\n",
        "    \"\"\"\n",
        "    # Combine X and y into a single matrix\n",
        "    A = np.hstack((X, y.reshape(-1, 1)))\n",
        "\n",
        "    # Singular Value Decomposition\n",
        "    U, S, Vt = np.linalg.svd(A, full_matrices=False)\n",
        "\n",
        "    # Last column of V (smallest singular value direction)\n",
        "    v = Vt.T[:, -1]\n",
        "\n",
        "    # Separate coefficients and intercept\n",
        "    coefs = -v[:-1] / v[-1]\n",
        "    return coefs\n",
        "\n",
        "def apply_tls(data, features, target):\n",
        "    \"\"\"\n",
        "    Apply Total Least Squares to a dataset.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): Dataset to analyze.\n",
        "        features (list): List of feature column names.\n",
        "        target (str): Target column name.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Estimated regression coefficients.\n",
        "    \"\"\"\n",
        "    X = data[features].values\n",
        "    y = data[target].values\n",
        "    return total_least_squares(X, y)\n",
        "\n",
        "# Prepare data\n",
        "features = best_features\n",
        "target = 'price'\n",
        "\n",
        "# TLS on filtered dataset\n",
        "cleaner_data_features = cleaner_data[features]\n",
        "cleaner_data_target = cleaner_data[target]\n",
        "cleaner_coefficients = apply_tls(cleaner_data, features, target)\n",
        "\n",
        "# TLS on original dataset\n",
        "original_data_features = house_rad[features]\n",
        "original_data_target = house_rad[target]\n",
        "original_coefficients = apply_tls(house_rad, features, target)\n",
        "\n",
        "# Predictions for performance comparison\n",
        "cleaner_predictions = np.dot(cleaner_data_features, cleaner_coefficients)\n",
        "original_predictions = np.dot(original_data_features, original_coefficients)\n",
        "\n",
        "# Evaluate performance\n",
        "cleaner_mse = mean_squared_error(cleaner_data_target, cleaner_predictions)\n",
        "cleaner_r2 = r2_score(cleaner_data_target, cleaner_predictions)\n",
        "\n",
        "original_mse = mean_squared_error(original_data_target, original_predictions)\n",
        "original_r2 = r2_score(original_data_target, original_predictions)\n",
        "\n",
        "# Results summary\n",
        "print(\"Filtered Dataset TLS Coefficients:\", cleaner_coefficients)\n",
        "print(\"Original Dataset TLS Coefficients:\", original_coefficients)\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "print(f\"Filtered Data - MSE: {cleaner_mse}, R2: {cleaner_r2}\")\n",
        "print(f\"Original Data - MSE: {original_mse}, R2: {original_r2}\")\n"
      ],
      "metadata": {
        "id": "mgqdnGYBTzTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import log\n",
        "\n",
        "cleaned_data = cleaner_data[cleaner_data['split'] == \"train\"]\n",
        "X = cleaned_data[best_features]\n",
        "# Add constant term for the intercept in the OLS model\n",
        "X_const = sm.add_constant(X)\n",
        "y_l = cleaned_data['price']\n",
        "\n",
        "# Check for zero or negative values in the target variable\n",
        "print(y_l[y_l <= 0])\n",
        "\n",
        "# Count zero or negative values\n",
        "print(f\"Number of zero or negative values: {(y_l <= 0).sum()}\")\n",
        "\n",
        "print(y_l.head())\n",
        "\n",
        "\n",
        "# Apply log transformation to target variable\n",
        "y_log = np.log(y_l + 1)  # Shift by 1 to avoid log(0)\n",
        "\n",
        "# Re-fit the model\n",
        "model_log = sm.OLS(y_log, X_const).fit()\n",
        "print(model_log.summary())\n",
        "\n",
        "# Check residuals again\n",
        "predictions_log = model_log.predict(X_const)\n",
        "residuals_log = y_log - predictions_log"
      ],
      "metadata": {
        "id": "9hk3fWT2k-vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filtered Dataset:** The coefficients for the filtered dataset suggest a more stable and possibly robust model. The magnitudes of the coefficients are relatively consistent, indicating that filtering likely removed influential outliers or noisy observations.\n",
        "\n",
        "**Original Dataset:** The coefficients for the original dataset vary significantly in magnitude. This can indicate that the presence of noisy or extreme observations influenced the model, leading to potentially less stable regression estimates."
      ],
      "metadata": {
        "id": "ChPn1G52Vq70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_coefficients(filtered, original, feature_names):\n",
        "    \"\"\"\n",
        "    Compare TLS coefficients from filtered and original datasets.\n",
        "\n",
        "    Args:\n",
        "        filtered (list or numpy.ndarray): Coefficients from the filtered dataset.\n",
        "        original (list or numpy.ndarray): Coefficients from the original dataset.\n",
        "        feature_names (list): Feature names corresponding to the coefficients.\n",
        "    \"\"\"\n",
        "    x = range(len(feature_names))\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(x, filtered, width=0.4, label='Filtered', align='center')\n",
        "    plt.bar([i + 0.4 for i in x], original, width=0.4, label='Original', align='center')\n",
        "    plt.xticks([i + 0.2 for i in x], feature_names, rotation=45, ha='right')\n",
        "    plt.title(\"Comparison of Coefficients\")\n",
        "    plt.xlabel(\"Features\")\n",
        "    plt.ylabel(\"Coefficient Value\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "compare_coefficients(cleaner_coefficients, original_coefficients, best_features)\n"
      ],
      "metadata": {
        "id": "WEM-TeW4Y0z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Question 10\n",
        "\n",
        "Explore additional robust regression approaches (e.g., Huber regression, quantile regression, or M-estimators) to estimate the regression coefficients on the filtered and full datasets. Compare the results across these methods, discussing the strengths and limitations of each approach in the context of predicting house prices in the King County area. How do these robust methods handle potential outliers or influential observations in the data?\n"
      ],
      "metadata": {
        "id": "3MKbRUYkb2mx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Huber regression"
      ],
      "metadata": {
        "id": "_L8srt_3V9dD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tools.tools import add_constant\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "\n",
        "y = cleaner_data['price']\n",
        "X = add_constant(cleaner_data.drop(columns=['split']))\n",
        "\n",
        "y_full = house_rad['price']\n",
        "X_full = add_constant(house_rad.drop(columns=['split', 'sqft_living', 'sqft_above']))\n",
        "\n",
        "\n",
        "# Fit Huber Regressor\n",
        "huber_model = HuberRegressor()\n",
        "huber_model.fit(X, y)\n",
        "\n",
        "# Predictions and performance\n",
        "y_pred_filtered = huber_model.predict(X)\n",
        "mse_filtered = mean_squared_error(y, y_pred_filtered)\n",
        "\n",
        "print(\"Huber Regression Coefficients (Filtered):\", huber_model.coef_)\n",
        "print(\"Huber Regression MSE (Filtered):\", mse_filtered)\n",
        "print()\n",
        "\n",
        "# Refit on the full dataset\n",
        "huber_model.fit(X_full, y_full)\n",
        "y_pred_full = huber_model.predict(X_full)\n",
        "mse_full = mean_squared_error(y_full, y_pred_full)\n",
        "\n",
        "print(\"Huber Regression Coefficients (Full):\", huber_model.coef_)\n",
        "print(\"Huber Regression MSE (Full):\", mse_full)"
      ],
      "metadata": {
        "id": "-WGaC7YOoaXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qualtile regression"
      ],
      "metadata": {
        "id": "UF9xh51aWCQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.regression.quantile_regression import QuantReg\n",
        "\n",
        "# Fit Quantile Regression for the median\n",
        "quantile_model = QuantReg(y, add_constant(X))\n",
        "quantile_results_filtered = quantile_model.fit(q=0.5)\n",
        "print(\"Quantile Regression Summary (Filtered):\")\n",
        "print(quantile_results_filtered.summary())\n",
        "\n",
        "# Refit on the full dataset\n",
        "quantile_model_full = QuantReg(y_full, add_constant(X_full))\n",
        "quantile_results_full = quantile_model_full.fit(q=0.5)\n",
        "print(\"Quantile Regression Summary (Full):\")\n",
        "print(quantile_results_full.summary())"
      ],
      "metadata": {
        "id": "3HTAC98NrClr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RLM"
      ],
      "metadata": {
        "id": "t2MXj93GWST2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.robust.robust_linear_model import RLM\n",
        "\n",
        "# Fit RLM using Huber's T norm\n",
        "rlm_model = RLM(y, add_constant(X))\n",
        "rlm_results_filtered = rlm_model.fit()\n",
        "print(\"RLM Summary (Filtered):\")\n",
        "print(rlm_results_filtered.summary())\n",
        "\n",
        "# Refit on the full dataset\n",
        "rlm_model_full = RLM(y_full, add_constant(X_full))\n",
        "rlm_results_full = rlm_model_full.fit()\n",
        "print(\"RLM Summary (Full):\")\n",
        "print(rlm_results_full.summary())\n"
      ],
      "metadata": {
        "id": "IVjd3zQ-rPSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Question 11\n",
        "\n",
        "Select the final model and compare the MSE and $R^2$ on the training, test, and validation datasets. What do these values suggest about the quality of the model and potential overfitting? Is your model suitable for predicting house prices in the King County area? If so, does this prediction have any limitations?\n",
        "\n"
      ],
      "metadata": {
        "id": "kHja9KSpb2tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "train_y = cleaner_data[cleaner_data['split']=='train']['price']\n",
        "train_y_hat = best_model.predict(cleaner_data[cleaner_data['split']=='train'])\n",
        "\n",
        "test_y = cleaner_data[cleaner_data['split']=='test']['price']\n",
        "test_y_hat = best_model.predict(cleaner_data[cleaner_data['split']=='test'])\n",
        "\n",
        "valid_y = cleaner_data[cleaner_data['split']=='validation']['price']\n",
        "valid_y_hat = best_model.predict(cleaner_data[cleaner_data['split']=='validation'])\n",
        "\n",
        "# Calculate R-squared\n",
        "r_squared_train = r2_score(train_y, train_y_hat)\n",
        "r_squared_test = r2_score(test_y, test_y_hat)\n",
        "r_squared_valid = r2_score(valid_y, valid_y_hat)\n",
        "\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse_train = mean_squared_error(train_y, train_y_hat)\n",
        "mse_test = mean_squared_error(test_y, test_y_hat)\n",
        "mse_valid = mean_squared_error(valid_y, valid_y_hat)\n",
        "\n",
        "print(f\"train: mse {mse_train}, R2 {r_squared_train}\")\n",
        "print(f\"test: mse {mse_test}, R2 {r_squared_test}\")\n",
        "print(f\"validation: mse {mse_valid}, R2 {r_squared_valid}\")"
      ],
      "metadata": {
        "id": "Vajj99RbrW0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Voluntary part: Machine Learning Approach\n",
        "\n",
        "\n",
        "### Question 12\n",
        "Apply machine learning-based linear regression methods, such as Ridge Regression, Lasso, or Elastic Net, to the dataset. Use the train-test split to evaluate model performance and focus on feature selection. Identify the most relevant features based on these methods and compare how the selected features impact the model's predictive performance. Discuss how regularization affects feature selection and the trade-offs between bias and variance in the context of house price prediction. Additionally, evaluate the stability of selected features across different methods and provide recommendations for choosing the optimal feature set.\n"
      ],
      "metadata": {
        "id": "ySe_Y1iFqwzk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5kse02wVrWfK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}